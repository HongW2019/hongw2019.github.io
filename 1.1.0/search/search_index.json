{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OAP Project Overview Optimized Analytics Package (OAP) for Spark platform is a open source project for multiple Spark optimizations and features driving by Intel and the community. Spark is powerful and well optimized in a lot of aspects. But we still face a few challenges for Spark to the next level performance. The JVM and row-based computing engine preventing Spark to be fully optimized for Intel hardware, for example AVX/AVX512, GPU The current implementation of key aspects, such as memory management & shuffle didn\u2019t consider the latest technology advancements, like PMEM The batch processing engine a lot of times cannot satisfy the need of queries with high performance requirement. OAP Project is targeted to optimize Spark in these aspects above, now it has 8 components including SQL DS Cache, Native SQL Engine, Arrow Data Source, OAP MLlib, PMem Spill, PMem Common, PMem Shuffle and Remote Shuffle. Guide Please refer to the total OAP project installation and development guide below. OAP Installation Guide OAP Developer Guide You can get more detailed information from each module web page of OAP Project below, or from the left sidebar navigation. SQL DS Cache Native SQL Engine OAP MLlib PMem Shuffle Remote Shuffle PMem Spill PMem Common","title":"Overview"},{"location":"#oap-project-overview","text":"Optimized Analytics Package (OAP) for Spark platform is a open source project for multiple Spark optimizations and features driving by Intel and the community. Spark is powerful and well optimized in a lot of aspects. But we still face a few challenges for Spark to the next level performance. The JVM and row-based computing engine preventing Spark to be fully optimized for Intel hardware, for example AVX/AVX512, GPU The current implementation of key aspects, such as memory management & shuffle didn\u2019t consider the latest technology advancements, like PMEM The batch processing engine a lot of times cannot satisfy the need of queries with high performance requirement. OAP Project is targeted to optimize Spark in these aspects above, now it has 8 components including SQL DS Cache, Native SQL Engine, Arrow Data Source, OAP MLlib, PMem Spill, PMem Common, PMem Shuffle and Remote Shuffle.","title":"OAP Project Overview"},{"location":"#guide","text":"Please refer to the total OAP project installation and development guide below. OAP Installation Guide OAP Developer Guide You can get more detailed information from each module web page of OAP Project below, or from the left sidebar navigation. SQL DS Cache Native SQL Engine OAP MLlib PMem Shuffle Remote Shuffle PMem Spill PMem Common","title":"Guide"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b branch-1.1-spark-3.x https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : branch-1.1-spark-3.x is for OAP modules version v1.1.0-spark-3.0.0 Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b branch-1.1-spark-3.x https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : branch-1.1-spark-3.x is for OAP modules version v1.1.0-spark-3.0.0 Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"Release-Cadence/","text":"Starting with OAP 1.1.0, the OAP project will follow the semantic versioning guidelines with a few deviations. These small differences account for OAP\u2019s nature as a multi-module project. OAP Versions Each OAP release will be versioned: [MAJOR].[FEATURE].[MAINTENANCE] Release Cadence In general, major feature releases occur about every 4 months. Hence, OAP would generally be released about 4 months after 1.1.0. Maintenance releases happen as needed in between feature releases. Major releases do not happen according to a fixed schedule. OAP 1.2 Release Window Date Event July 23th 2021 Code freeze. Release branch cut. Mid August 2021 QA period. Focus on bug fixes, tests, stability and docs. Generally, no new features merged. August 2021 Release candidates (RC), etc. until final release passes Maintenance Releases Feature release branches will, generally, be maintained with bug fix releases for a period of several months.","title":"Release Cadence"},{"location":"Release-Cadence/#oap-versions","text":"Each OAP release will be versioned: [MAJOR].[FEATURE].[MAINTENANCE]","title":"OAP Versions"},{"location":"Release-Cadence/#release-cadence","text":"In general, major feature releases occur about every 4 months. Hence, OAP would generally be released about 4 months after 1.1.0. Maintenance releases happen as needed in between feature releases. Major releases do not happen according to a fixed schedule.","title":"Release Cadence"},{"location":"Release-Cadence/#oap-12-release-window","text":"Date Event July 23th 2021 Code freeze. Release branch cut. Mid August 2021 QA period. Focus on bug fixes, tests, stability and docs. Generally, no new features merged. August 2021 Release candidates (RC), etc. until final release passes","title":"OAP 1.2 Release Window"},{"location":"Release-Cadence/#maintenance-releases","text":"Feature release branches will, generally, be maintained with bug fix releases for a period of several months.","title":"Maintenance Releases"}]}